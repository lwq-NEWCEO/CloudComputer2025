import os
import re
import json
import hashlib
from pathlib import Path
from typing import List, Dict, Any

import fitz  # PyMuPDF
import pandas as pd
import camelot

import torch
from PIL import Image
from transformers import AutoProcessor, AutoModelForVision2Seq

# ------------------------
# Config
# ------------------------
# å‡è®¾è„šæœ¬åœ¨ rag/scripts/ ä¸‹ï¼ŒBASE_DIR æŒ‡å‘ rag/
BASE_DIR = Path(__file__).resolve().parent.parent
DOCS_DIR = BASE_DIR / "docs"
OUT_JSONL = BASE_DIR / "data" / "parsed_docs.jsonl"
IMG_DIR = BASE_DIR / "data" / "extracted_images"

VLM_NAME = "Qwen/Qwen2-VL-2B-Instruct"


# ------------------------
# Utils
# ------------------------
def write_jsonl(records, path: Path):
    path.parent.mkdir(parents=True, exist_ok=True)
    with open(path, "w", encoding="utf-8") as f:
        for r in records:
            f.write(json.dumps(r, ensure_ascii=False) + "\n")


# ------------------------
# VLM (ä¿æŒä¸å˜)
# ------------------------
def load_vlm():
    if not torch.cuda.is_available():
        return None, None
    try:
        print(f"ğŸ”„ Loading VLM: {VLM_NAME}...")
        processor = AutoProcessor.from_pretrained(VLM_NAME, trust_remote_code=True, use_fast=False)
        model = AutoModelForVision2Seq.from_pretrained(VLM_NAME, torch_dtype=torch.float16, device_map="auto",
                                                       trust_remote_code=True)
        return processor, model
    except:
        return None, None


def vlm_caption(processor, model, image_path: Path) -> str:
    if not processor or not model: return ""
    try:
        image = Image.open(image_path).convert("RGB")
        instruction = "ä½ æ˜¯å…¨èƒ½çŸ¥è¯†åº“åŠ©æ‰‹ã€‚å¦‚æœæ˜¯ä»£ç å›¾è¯·æå–é€»è¾‘ï¼›å¦‚æœæ˜¯æ¶æ„å›¾è¯·æè¿°æµç¨‹ï¼›å¦‚æœæ˜¯å›¾è¡¨è¯·æå–ç»“è®ºã€‚"
        messages = [{"role": "user", "content": [{"type": "image"}, {"type": "text", "text": instruction}]}]
        prompt = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
        inputs = processor(text=prompt, images=image, return_tensors="pt").to(model.device)
        with torch.no_grad():
            out = model.generate(**inputs, max_new_tokens=512, do_sample=False)
        return processor.batch_decode(out, skip_special_tokens=True)[0].strip()
    except:
        return ""


# ------------------------
# PDF Extraction
# ------------------------
def extract_text_pymupdf(pdf_path: Path):
    try:
        doc = fitz.open(pdf_path)
        for page_idx in range(len(doc)):
            text = doc[page_idx].get_text("text").strip()
            if text:
                yield {
                    "type": "text",
                    "source": pdf_path.name,
                    "pdf_name": pdf_path.name,
                    "page": page_idx,
                    "content": f"ã€æ¥æºPDFã€‘{pdf_path.name}\n{text}",
                    "meta": {"is_pdf": True}
                }
    except:
        pass


# ------------------------
# Markdown Processing (æ ¸å¿ƒä¿®å¤ç‰ˆ)
# ------------------------
def process_markdown_file(md_path: Path, force_difficulty: str = "Unknown") -> List[Dict[str, Any]]:
    records = []
    try:
        with open(md_path, "r", encoding="utf-8") as f:
            content = f.read()

        # 1. è·å–åŸºæœ¬ä¿¡æ¯
        folder_name = md_path.parent.name
        lines = content.split('\n')
        title = lines[0].strip().replace("# ", "") if lines else folder_name

        # 2. æ„å»ºå¢å¼ºæ–‡æœ¬
        full_text = f"""
ã€çŸ¥è¯†æ¡ç›®ã€‘ LeetCode / ç®—æ³•ç¬”è®°
ã€é¢˜ç›®åç§°ã€‘ {title}
ã€é¢˜ç›®éš¾åº¦ã€‘ {force_difficulty}
ã€æ¥æºè·¯å¾„ã€‘ {folder_name}/{md_path.name}
--------------------------------------------------
{content}
""".strip()

        # 3. å›¾ç‰‡å¼•ç”¨å¤„ç† (å…³é”®ä¿®å¤ï¼šå¿½ç•¥ç½‘ç»œå›¾ç‰‡)
        img_matches = re.finditer(r'!\[(.*?)\]\((.*?)\)', content)
        for match in img_matches:
            alt, rel = match.groups()

            final_path = ""

            # === ä¿®å¤ Start: é‡åˆ°ç½‘ç»œå›¾ç‰‡ç›´æ¥ä¿ç•™ï¼Œä¸æ‹¼æœ¬åœ°è·¯å¾„ ===
            if rel.startswith(("http:", "https:")):
                final_path = rel
            else:
                # åªæœ‰æœ¬åœ°å›¾ç‰‡æ‰è¿›è¡Œè·¯å¾„è§£æ
                try:
                    abs_img = (md_path.parent / rel).resolve()
                    if str(abs_img).startswith(str(BASE_DIR)):
                        final_path = str(abs_img.relative_to(BASE_DIR))
                    else:
                        final_path = str(abs_img)
                except:
                    # å¦‚æœè§£ææœ¬åœ°è·¯å¾„å¤±è´¥ï¼Œè·³è¿‡è¯¥å›¾ç‰‡
                    continue
            # === ä¿®å¤ End ===

            records.append({
                "type": "figure",
                "source": md_path.name,
                "page": 0,
                "image_path": final_path,
                "content": f"ã€å›¾ç‰‡ã€‘{title}: {alt}",
                "meta": {"difficulty": force_difficulty}
            })

        # 4. ç”Ÿæˆè®°å½•
        try:
            rel_path = str(md_path.relative_to(BASE_DIR))
        except:
            rel_path = str(md_path)

        records.append({
            "type": "text",
            "source": md_path.name,
            "pdf_name": md_path.name,
            "page": 0,
            "content": full_text,
            "meta": {
                "difficulty": force_difficulty,
                "title": title,
                "file_path": rel_path,
                "is_leetcode": True
            }
        })
        return records

    except Exception as e:
        print(f"âŒ è§£æå¤±è´¥: {md_path} - {e}")
        return []


# ------------------------
# Main Logic (ç²¾å‡†æ‰«æ)
# ------------------------
def main():
    if not DOCS_DIR.exists():
        print(f"âŒ æ‰¾ä¸åˆ° docs ç›®å½•: {DOCS_DIR}")
        return

    processor, model = load_vlm()
    records = []

    # 1. æ‰«æ PDF
    print("ğŸ” æ­£åœ¨æ‰«æ PDF...")
    for pdf in DOCS_DIR.rglob("*.pdf"):
        records.extend(list(extract_text_pymupdf(pdf)))

    # 2. æ‰«æ LeetCode (Easy/Mid/Hard)
    # æ˜ å°„è¡¨ï¼šæ–‡ä»¶å¤¹å -> éš¾åº¦æ ‡è®°
    target_dirs = {
        "easy": "Easy",
        "mid": "Medium",
        "midium": "Medium",  # å…¼å®¹ä½ çš„ midium æ‹¼å†™
        "medium": "Medium",
        "hard": "Hard"
    }

    print("\nğŸ” æ­£åœ¨æ‰«æ LeetCode é¢˜ç›®ç›®å½•...")

    for dir_name, difficulty in target_dirs.items():
        target_path = DOCS_DIR / dir_name
        if not target_path.exists():
            continue

        print(f"   ğŸ“‚ è¿›å…¥ç›®å½•: {dir_name} (éš¾åº¦: {difficulty})")

        # éå†è¯¥éš¾åº¦ä¸‹çš„æ‰€æœ‰å­æ–‡ä»¶å¤¹
        sub_count = 0
        for sub_folder in target_path.iterdir():
            if sub_folder.is_dir():
                # ç²¾å‡†æ‰¾ README.md
                readme_path = sub_folder / "README.md"
                if readme_path.exists():
                    new_recs = process_markdown_file(readme_path, force_difficulty=difficulty)
                    records.extend(new_recs)
                    sub_count += 1

        print(f"      -> å·²è§£æ {sub_count} é¢˜")

    # 3. æ‰«æäººå·¥æ–‡æ¡£ (Manual)
    manual_dir = DOCS_DIR / "manual"
    if manual_dir.exists():
        print(f"\nğŸ” æ­£åœ¨æ‰«æäººå·¥æ–‡æ¡£ (manual)...")
        m_count = 0
        for md in manual_dir.rglob("*.md"):
            new_recs = process_markdown_file(md, force_difficulty="Manual")
            records.extend(new_recs)
            m_count += 1
        print(f"      -> å·²è§£æ {m_count} ä¸ªæ–‡æ¡£")

    # æ±‡æ€»
    print(f"\nâœ… æ‰«æç»“æŸï¼æ€»å…±ç”Ÿæˆ {len(records)} æ¡æ•°æ®è®°å½•ã€‚")
    print(f"ğŸ’¾ å†™å…¥æ–‡ä»¶: {OUT_JSONL}")
    write_jsonl(records, OUT_JSONL)
    print("ğŸš€ è¯·è¿è¡Œ python build_index_ollama.py æ„å»ºç´¢å¼•")


if __name__ == "__main__":
    main()
