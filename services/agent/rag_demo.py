import os
import logging
import requests
import chromadb
import torch
import traceback
from pathlib import Path
from typing import Optional, Dict, Any, List

from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from pydantic import BaseModel
from chromadb.config import Settings
# from FlagEmbedding import FlagReranker
from neo4j import GraphDatabase

# ================= 1. å…³é”®è·¯å¾„é…ç½® =================
CURRENT_SCRIPT_PATH = Path(__file__).resolve()
PROJECT_ROOT = CURRENT_SCRIPT_PATH.parent.parent.parent

CHROMA_DIR = Path(os.getenv("CHROMA_DIR", PROJECT_ROOT / "index" / "chroma"))
CHROMA_COLLECTION = os.getenv("CHROMA_COLLECTION", "rag_docs_ollama")
DATA_DIR = Path(os.getenv("DATA_DIR", PROJECT_ROOT / "data"))
DOCS_DIR = Path(os.getenv("DOCS_DIR", PROJECT_ROOT / "docs"))
MANUAL_DIR = DOCS_DIR / "manual"

# ğŸŒŸ è®¾ç½®ç«¯å£ä¸º 8088 (åŒ¹é…å‰ç«¯)ï¼ŒHost è®¾ç½®ä¸º 127.0.0.1
CURRENT_PORT = 8088
HOST_URL = f"http://127.0.0.1:{CURRENT_PORT}"

# ================= 2. æ¨¡å‹é…ç½® =================
_raw_host = os.getenv("OLLAMA_HOST", "http://127.0.0.1:11435")
if not _raw_host.startswith("http"): _raw_host = f"http://{_raw_host}"
OLLAMA_HOST = _raw_host

OLLAMA_LLM_MODEL = "qwen2.5:7b-instruct"
OLLAMA_EMBED_MODEL = "nomic-embed-text"
USE_RERANKER = False

# âš ï¸âš ï¸âš ï¸ è¿™é‡Œå¿…é¡»å¡«å¯¹æ‚¨çš„ Neo4j å¯†ç ï¼âš ï¸âš ï¸âš ï¸
# å¦‚æœä¹‹å‰çš„æŠ¥é”™æ˜¯ Unauthorizedï¼Œè¯´æ˜ "password123" æ˜¯é”™çš„
NEO4J_URI = "bolt://localhost:7687"
NEO4J_USER = "neo4j"
NEO4J_PASSWORD = "12345678"

TOPK_RECALL = 15
TOPK_CONTEXT = 5

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI(title="RAG Service Optimized")

app.add_middleware(
    CORSMiddleware, allow_origins=["*"], allow_credentials=True, allow_methods=["*"], allow_headers=["*"],
)

if DATA_DIR.exists():
    app.mount("/assets", StaticFiles(directory=str(DATA_DIR)), name="assets")
if DOCS_DIR.exists():
    app.mount("/doc_assets", StaticFiles(directory=str(DOCS_DIR)), name="doc_assets")


class AskRequest(BaseModel):
    question: str
    where: Optional[Dict[str, Any]] = None


class AskResponse(BaseModel):
    answer: str
    citations: List[Dict[str, Any]]


class GraphData(BaseModel):
    nodes: List[Dict]
    links: List[Dict]


# ================= å·¥å…·å‡½æ•° =================

def convert_path_to_url(local_path: str) -> str:
    if not local_path: return ""
    p = str(local_path).replace("\\", "/")
    if p.startswith("http"): return p
    if p.startswith("docs/"):
        return f"{HOST_URL}/doc_assets/{p[5:]}"
    elif "data/" in p:
        rel = p.split("data/")[-1]
        return f"{HOST_URL}/assets/{rel}"
    return f"{HOST_URL}/doc_assets/{p}"


def fix_markdown_images(text: str) -> str:
    if not text: return ""
    text = text.replace("](../pic/", f"]({HOST_URL}/doc_assets/pic/")
    text = text.replace("](.../pic/", f"]({HOST_URL}/doc_assets/pic/")
    text = text.replace("](docs/pic/", f"]({HOST_URL}/doc_assets/pic/")
    text = text.replace("](./pic/", f"]({HOST_URL}/doc_assets/pic/")
    return text


def ollama_embed(text: str):
    try:
        r = requests.post(f"{OLLAMA_HOST}/api/embeddings",
                          json={"model": OLLAMA_EMBED_MODEL, "prompt": text}, timeout=10)
        return r.json().get("embedding")
    except:
        return []


def ollama_generate(prompt: str):
    try:
        payload = {
            "model": OLLAMA_LLM_MODEL,
            "prompt": prompt,
            "stream": False,
            "options": {"temperature": 0.3, "num_ctx": 4096}
        }
        r = requests.post(f"{OLLAMA_HOST}/api/generate", json=payload, timeout=120)
        return r.json().get("response", "").strip()
    except Exception as e:
        return f"LLM Error: {e}"


def build_prompt(question: str, contexts: List[str]) -> str:
    fixed_contexts = [fix_markdown_images(c) for c in contexts]
    context_str = "\n\n".join([f"ã€å·²çŸ¥ä¿¡æ¯ {i + 1}ã€‘\n{c}" for i, c in enumerate(fixed_contexts)])
    return f"""
ä½ æ˜¯ä¸€ä¸ªç¼–ç¨‹è¾…å¯¼åŠ©æ‰‹ã€‚è¯·åŸºäºå·²çŸ¥ä¿¡æ¯å›ç­”ã€‚

ã€å·²çŸ¥ä¿¡æ¯ã€‘ï¼š
{context_str}

**è¦æ±‚**ï¼š
1. è‹¥ç”¨æˆ·è¦æ±‚å‡ºé¢˜ï¼Œè¯·è®¾è®¡é¢˜ç›®ã€‚
2. ä¿ç•™å›¾ç‰‡é“¾æ¥ã€‚

**é—®é¢˜**ï¼š
{question}
""".strip()


def doc_to_context(doc: str, md: Dict[str, Any]) -> str:
    src = md.get("pdf_name", "") or md.get("source", "")
    header = f"[æ¥æº: {src}]"
    if md.get("image_path"):
        header += f"\n[å‚è€ƒå›¾ç‰‡]: ![]({convert_path_to_url(md['image_path'])})"
    return header + "\n" + doc


# ================= å…¨å±€å®ä¾‹ =================
chroma_client = None
collection = None
neo4j_driver = None


@app.on_event("startup")
def startup():
    global chroma_client, collection, neo4j_driver
    print("\n" + "=" * 50)
    print("ğŸš€ ç³»ç»Ÿå¯åŠ¨è‡ªæ£€ä¸­...")

    if MANUAL_DIR.exists():
        print(f"âœ… å‘ç° Manual ç›®å½•: {MANUAL_DIR}")
    else:
        print(f"âŒ è­¦å‘Š: Manual ç›®å½•ä¸å­˜åœ¨: {MANUAL_DIR}")

    try:
        chroma_client = chromadb.PersistentClient(path=str(CHROMA_DIR), settings=Settings(anonymized_telemetry=False))
        collection = chroma_client.get_or_create_collection(CHROMA_COLLECTION)
        print("âœ… ChromaDB è¿æ¥æˆåŠŸ")
    except:
        print("âŒ ChromaDB è¿æ¥å¤±è´¥")

    # Neo4j è¿æ¥é€»è¾‘
    try:
        neo4j_driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASSWORD))
        # å°è¯•éªŒè¯è¿æ¥ï¼Œå¦‚æœå¯†ç é”™è¯¯è¿™é‡Œä¼šæŠ¥é”™
        neo4j_driver.verify_connectivity()
        print("âœ… Neo4j è¿æ¥æˆåŠŸ")
    except Exception as e:
        print(f"âš ï¸ Neo4j è¿æ¥å¤±è´¥ (ä¸å½±å“èŠå¤©åŠŸèƒ½): {e}")
        print("ğŸ‘‰ è¯·æ£€æŸ¥ä»£ç ä¸­ NEO4J_PASSWORD æ˜¯å¦æ­£ç¡®ï¼")

    print("=" * 50 + "\n")


@app.on_event("shutdown")
def shutdown():
    if neo4j_driver: neo4j_driver.close()


# ================= 3. API æ¥å£ =================

@app.post("/ask", response_model=AskResponse)
def ask(req: AskRequest):
    q = req.question.strip()
    q_lower = q.lower()
    if not q: return {"answer": "Empty question", "citations": []}

    quiz_keywords = ["å‡ºé¢˜", "è€ƒè€ƒ", "generate", "quiz", "test me", "exercise"]
    is_quiz = any(k in q_lower for k in quiz_keywords)
    lcs_keywords = ["lcs", "æœ€é•¿å…¬å…±å­åºåˆ—"]
    is_lcs = any(k in q_lower for k in lcs_keywords)

    print(f"\nğŸ” [Ask] '{q}' -> is_lcs={is_lcs}, is_quiz={is_quiz}")

    # åœºæ™¯ A: ç›´é€šè½¦
    if is_lcs and not is_quiz:
        manual_file = MANUAL_DIR / "LCS_Solution.md"
        if not manual_file.exists():
            return {"answer": f"âš ï¸ æ–‡ä»¶æœªæ‰¾åˆ°: {manual_file}", "citations": []}

        print("ğŸš€ [Route] ç›´é€š Manual æ–‡ä»¶")
        raw_content = manual_file.read_text(encoding="utf-8")
        return {
            "answer": fix_markdown_images(raw_content),
            "citations": [{"evidence_id": 1, "score": 1.0, "source": "LCS_Solution.md", "type": "manual"}]
        }

    # åœºæ™¯ B: RAG
    print("ğŸ¤– [Route] æ ‡å‡† RAG (Ollama)")
    q_emb = ollama_embed(q)
    if not q_emb: return {"answer": "Embedding failed", "citations": []}

    res = collection.query(query_embeddings=[q_emb], n_results=TOPK_RECALL, where=req.where)
    docs = res.get("documents", [[]])[0]
    metas = res.get("metadatas", [[]])[0]

    if not docs: return {"answer": "æœªæ‰¾åˆ°ç›¸å…³å†…å®¹ã€‚", "citations": []}

    candidates = [{"doc": d, "meta": m or {}} for d, m in zip(docs, metas)]
    top = candidates[:TOPK_CONTEXT]
    contexts = [doc_to_context(t["doc"], t["meta"]) for t in top]
    prompt = build_prompt(q, contexts)
    ans = ollama_generate(prompt)

    citations = []
    for i, item in enumerate(top):
        citations.append({
            "evidence_id": i + 1,
            "score": 0.9,
            "source": item["meta"].get("pdf_name", "unknown"),
            "image_url": convert_path_to_url(item["meta"].get("image_path", "")),
            "type": "text"
        })

    return {"answer": ans, "citations": citations}


# ğŸŒŸğŸŒŸğŸŒŸ è¡¥å…¨ Graph æ¥å£ (å¿…é¡»æœ‰è¿™ä¸ªå‰ç«¯æ‰èƒ½æ˜¾ç¤ºå›¾è°±) ğŸŒŸğŸŒŸğŸŒŸ
@app.get("/graph/overview", response_model=GraphData)
def graph_overview(limit: int = 100):
    # å¦‚æœé©±åŠ¨æ²¡è¿ä¸Šï¼Œç›´æ¥è¿”å›ç©º
    if not neo4j_driver:
        print("âŒ [Graph] Neo4j driver is None (Authentication failed?)")
        return {"nodes": [], "links": []}

    try:
        print(f"ğŸ” [Graph] Querying Neo4j (Limit {limit})...")
        with neo4j_driver.session() as session:
            # å…¼å®¹ä¸åŒç‰ˆæœ¬çš„ Neo4j é©±åŠ¨è¿”å›
            res = session.run(f"MATCH (n)-[r]->(m) RETURN n,r,m LIMIT {limit}")
            nodes = {}
            links = []

            for rec in res:
                n, m, r = rec['n'], rec['m'], rec['r']

                # å®‰å…¨è·å– ID
                n_id = str(n.element_id) if hasattr(n, 'element_id') else str(n.id)
                m_id = str(m.element_id) if hasattr(m, 'element_id') else str(m.id)

                # å®‰å…¨è·å– Label
                n_lbl = list(n.labels)[0] if n.labels else "Entity"
                m_lbl = list(m.labels)[0] if m.labels else "Entity"

                nodes[n_id] = {"id": n_id, "label": n_lbl, "name": n.get("name", n_lbl)}
                nodes[m_id] = {"id": m_id, "label": m_lbl, "name": m.get("name", m_lbl)}
                links.append({"source": n_id, "target": m_id, "type": r.type})

            print(f"âœ… [Graph] Success! Found {len(nodes)} nodes, {len(links)} links.")
            return {"nodes": list(nodes.values()), "links": links}

    except Exception as e:
        print(f"âŒ [Graph Error]: {e}")
        traceback.print_exc()
        return {"nodes": [], "links": []}


if __name__ == "__main__":
    import uvicorn

    # ç»‘å®š 0.0.0.0 å’Œ ç«¯å£ 8088
    print(f"ğŸš€ æœåŠ¡å¯åŠ¨ä¸­: http://127.0.0.1:{CURRENT_PORT}")
    uvicorn.run(app, host="0.0.0.0", port=CURRENT_PORT)
